---
key: 4bd7391785ff50b8ceb87e67f1397a6e
title: 'Insert title here'
---

## Tree-based imputation

```yaml
type: TitleSlide
key: 3970c4b236
```

`@lower_third`
name: Michał Oleszak
title: Machine Learning Engineer

`@script`
In the last two lessons, you have learned to use simple statistical models to impute missing values. Now let's move on to discuss a different kind of models.

---

## Tree-based imputation approach

```yaml
type: FullSlide
key: ee5a81dc7c
disable_transition: false
hide_title: false
```

`@part1`
Idea: use machine learning models based on decision trees to predict missing values{{1}}
* Non-parametric approach: no assumptions on relationships between variables{{2}}
* Can pick up complex non-linear patterns{{3}}
* Often better predictive performance compared to simple statistical models{{4}}

> 

This course: `missForest` package, based on `randomForest`{{5}}

`@script`
Traditional statistical models can be replaced by machine learning models to predict missing values.

---

## Recap: random forests

```yaml
type: FullSlide
key: 54bdbd4333
```

`@part1`
* **Decision trees**:{{1}}
	* Choose a variable and split the data into two or more subsets based on its values {{2}}
    * For each subset, choose another variable and again use it for splitting {{3}}
    * Continue splitting, until each subset contains a unique target value {{4}}
    * Splitter variables and thresholds at each step are chosen such that the target variable in each subset is as homogeneous as possible {{5}}

* **Random forests**: {{6}}
	* Take a sample from the data with replacement _(bagging)_ {{7}}
    * Fit a decision tree, considering only a few random variables to split on at each step {{8}}
    * Repeat multiple times and aggregate the results {{9}}

`@script`
trees protect each other from their individual errors

---

## missForest algorithm

```yaml
type: FullSlide
key: 067795754d
```

`@part1`
1. Make an initial guess for missing values with mean imputation {{1}}
2. Sort variables by the increasing amount of missing values {{2}}
3. While not **STOP**: {{3}}
	* Store previously imputed data matrix {{4}}
	* For each variable **x** (in the sorted order): {{5}}
        * Fit a random forest to predict **x** with other variables, using data where **x** is observed {{6}}
        * Predict missing values in **x** with the trained forest {{7}}
        * Update the imputed data matrix with predictions of **x** {{8}}

**STOP** when the difference between the last two imputed data matrices increases for the first time.{{9}}

`@script`
To begin, make an initial guess for the missing values in X using mean imputation or another imputation method. Then, sort the variables Xs, s = 1, . . . , p according to the amount of missing values
starting with the lowest amount. For each variable Xs the missing values are imputed by first fitting a
random forest with response y
(s)
obs and predictors x
(s)
obs; then, predicting the missing values y
(s)
mis by applying the trained random forest to x
(s)
mis. The imputation procedure is repeated until a stopping criterion
is met.

The stopping criterion is met when the difference between the newly imputed data matrix and the previous one increases for the first time.

---

## missForest example

```yaml
type: TwoColumns
key: 1f0fd38014
```

`@part1`
Introduce missing values to _iris_ data: {{1}}
```
> data("iris")
> iris_miss <- prodNA(iris, 
                      noNA = 0.1)
```
{{1}}
```
> sum(is.na(iris_miss))
[1] 75
```
{{2}}

`@part2`
Impute with _missForest_: {{3}}
```
> library(missForest)
> imp_list <- missForest(iris_miss)
> iris_imp <- imp_list$ximp
```
{{3}}
```
> sum(is.na(iris_imp))
[1] 0 
```
{{4}}

`@script`
Let's see how missForest works in practice. First, we load the "iris" data set and remove a random 10% of data with the "prodNA" function. A quick check shows that there are 75 missing values. To impute them, we call the "missForest" function on the incomplete data frame. Note that this yield a list - we have to extract the imputed data with "$ximp".

---

## Imputation error

```yaml
type: FullSlide
key: cc9cfe1706
```

`@part1`
_missForest_ provides an out-of-bag (OOB) imputation error estimate:{{1}}
* normalized root mean squared error (NRMSE) for continuous variables{{2}}
* proportion of falsely classified entries (PFC) for categorical variables
{{3}}
> 

In both cases good performance leads to a value close to 0 and values around 1 indicate poor performance.{{4}}

```
> imp_list$OOBerror
```
{{5}}
```
     NRMSE        PFC
0.14357394 0.02962963
```
{{5}}

`@script`
In order to assess the quality of imputation, we can look at the out-of-bag estimate of imputation error based on the underlying random forests. It is defined as the normalized root mean squared error for continuous variables and the percentage of incorrect classifications for categorical variables. In both cases, the closer the values is to zero, the better. The errors can be extracted from missForest's output with "$OOBerror".

---

## Speed-accuracy trade-off

```yaml
type: FullSlide
key: 4b8868da1b
```

`@part1`
Growing a random forest for each variable for many iterations might take a lot of time for large datasets. There are two ways to speed up the process:{{1}}
* Reduce the number of trees grown in each forest (`ntree` argument){{2}}
* Reduce the number of variables randomly sampled at each split (`mtry` argument){{3}}

> 

Both are likely to decrease imputation accuracy, but differ in their effect on computation time:{{4}}
* Reducing `ntree` has linear effect on computation time{{5}}
* Reducing `mtry` gives a larger speed-up when with many variables{{6}}

`@script`


---

## Speed-accuracy trade-off example

```yaml
type: TwoColumns
key: b8a197e378
code_zoom: 80
```

`@part1`
Default settings:{{1}}
```
> start_time <- Sys.time()
> imp_list <- missForest(iris_miss)
> end_time <- Sys.time()

> print(imp_list$OOBerror)
> print(end_time - start_time)
```
{{1}}
```
     NRMSE        PFC
0.14357394 0.02962963
Time difference of 0.498292 secs
```
{{2}}

`@part2`
Reduced forests:{{3}}
```
> start_time <- Sys.time()
> imp_list <- missForest(iris_miss,
                         ntree = 25,
                         mtry = 2)
> end_time <- Sys.time()

> print(imp_list$OOBerror)
> print(end_time - start_time)
```
{{3}}
```
     NRMSE        PFC
0.15010153 0.05185185
Time difference of 0.193445 secs
```
{{4}}

`@script`
The default values are `ntree = 100` and `mtry = √p`,
where _p_ is the number of variables

---

## Let's practice!

```yaml
type: FinalSlide
key: dfa7d1547f
```

`@script`
