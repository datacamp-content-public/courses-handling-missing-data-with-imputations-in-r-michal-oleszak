---
key: 4bd7391785ff50b8ceb87e67f1397a6e
title: 'Insert title here'
---

## Tree-based imputation

```yaml
type: TitleSlide
key: 3970c4b236
```

`@lower_third`
name: MichaÅ‚ Oleszak
title: Machine Learning Engineer

`@script`
In the last two lessons, you have learned to use simple statistical models to impute missing values. Now let's move on to discuss a different kind of models.

---

## Tree-based imputation approach

```yaml
type: FullSlide
key: ee5a81dc7c
disable_transition: false
hide_title: false
```

`@part1`
Idea: use machine learning models based on decision trees to predict missing values{{1}}
* Non-parametric approach: no assumptions on relationships between variables{{2}}
* Can pick up complex non-linear patterns{{2}}
* Often better predictive performance compared to simple statistical models{{2}}

> 

This course: `missForest` package, based on `randomForest`{{3}}

`@script`
Traditional statistical models can be replaced by machine learning models to predict missing values. This approach has its advantages: it doesn't impose any assumptions on relationships between variables and is able to pick up complex, non-linear patterns, which often leads to a better predictive performance. In this course, we will use the "missForest" package, which uses "randomForest" behind the hood.

---

## Recap: decision trees and random forests

```yaml
type: FullSlide
key: 54bdbd4333
```

`@part1`
* **Decision trees**:{{1}}
	* Choose a variable and split the data into two or more subsets based on its values {{1}}
    * For each subset, choose another variable and again use it for splitting {{1}}
    * Continue splitting, until each subset contains a unique target value {{1}}
    * Splitter variables and thresholds at each step are chosen such that the target variable in each subset is as homogeneous as possible {{1}}

* **Random forests**: {{2}}
	* Take a sample from the data with replacement _(bagging)_ {{2}}
    * Fit a decision tree, considering only a few random variables to split on at each step {{2}}
    * Repeat multiple times and aggregate the results {{2}}

`@script`
Let's start with a short and simple recap of decisions trees, the basis of random forests. Decision trees are built by continuously splitting the data into subsets based on the values of selected variables. The splitting continues until each subset has a unique value of the target variable. The splitter variables and thresholds are chosen such that the target in each subset is as homogeneous as possible. 
Random forest are collections of many trees, based on resampled data (procedure known as bagging) and considering only a random selection of variables to split on. The results from multiple trees are aggregated at the end.

---

## missForest algorithm

```yaml
type: FullSlide
key: 067795754d
```

`@part1`
1. Make an initial guess for missing values with mean imputation {{1}}
2. Sort variables by the increasing amount of missing values {{2}}
3. While not **STOP**: {{3}}
	* Store previously imputed data matrix {{3}}
	* For each variable **x** (in the sorted order): {{3}}
        * Fit a random forest to predict **x** with other variables, using data where **x** is observed {{3}}
        * Predict missing values in **x** with the trained forest {{3}}
        * Update the imputed data matrix with predictions of missing part of **x** {{3}}

**STOP** when the difference between the last two imputed data matrices increases for the first time.{{4}}

`@script`
Now that you know how random forests work, let's walk through the missForest algorithm. First, we make an initial guess for the missing values using mean imputation. Then, we sort the variables increasingly, according to the amount of missing values. For each variable, the missing values are imputed by first fitting a random forest to its observed part and then applying the trained forest to predict the missing part. The procedure is repeated until the stopping criterion is met, i.e. until the difference between the newly imputed data matrix and the previous one increases for the first time.

---

## missForest example

```yaml
type: TwoColumns
key: 1f0fd38014
```

`@part1`
Introduce missing values to _iris_ data: {{1}}
```
> data("iris")
> iris_miss <- prodNA(iris, 
                      noNA = 0.1)
```
{{1}}
```
> sum(is.na(iris_miss))
[1] 75
```
{{2}}

`@part2`
Impute with _missForest_: {{3}}
```
> library(missForest)
> imp_list <- missForest(iris_miss)
> iris_imp <- imp_list$ximp
```
{{3}}
```
> sum(is.na(iris_imp))
[1] 0 
```
{{4}}

`@script`
Let's see how missForest works in practice. First, we load the "iris" data set and remove a random 10% of data points with the "prodNA" function. A quick check shows that there are 75 missing values. To impute them, we call the "missForest" function on the incomplete data frame. Note that this yields a list - we have to extract the imputed data with "$ximp" notation. There are indeed no missing values in the imputed data.

---

## Imputation error

```yaml
type: FullSlide
key: cc9cfe1706
```

`@part1`
_missForest_ provides an out-of-bag (OOB) imputation error estimate:{{1}}
* normalized root mean squared error (NRMSE) for continuous variables{{1}}
* proportion of falsely classified entries (PFC) for categorical variables
{{1}}
> 

In both cases good performance leads to a value close to 0 and values around 1 indicate poor performance.{{1}}

```
> imp_list$OOBerror
```
{{2}}
```
     NRMSE        PFC
0.14357394 0.02962963
```
{{2}}

`@script`
In order to assess the quality of imputation, we can look at the out-of-bag estimate of imputation error based on the underlying random forests. It is defined as the normalized root mean squared error for continuous variables and the percentage of incorrect classifications for categorical variables. In both cases, the closer the values is to zero, the better. The errors can be extracted from missForest's output with "$OOBerror" notation.

---

## Speed-accuracy trade-off

```yaml
type: FullSlide
key: 4b8868da1b
```

`@part1`
Growing a random forest for each variable for many iterations might take a lot of time for large datasets. There are two ways to speed up the process:{{1}}
* Reduce the number of trees grown in each forest (`ntree` argument){{1}}
* Reduce the number of variables randomly sampled at each split (`mtry` argument){{1}}

> 

Both are likely to decrease imputation accuracy, but differ in their effect on computation time:{{2}}
* Reducing `ntree` has linear effect on computation time{{2}}
* Reducing `mtry` gives a larger speed-up when with many variables{{2}}

`@script`
Growing multiple random forests can be time-consuming, especially for large data sets. To decrease computation time, we can sacrifice some accuracy and reduce the forest size: either by growing less trees (set with "ntree" argument), or by considering less variables for splits inside the decision trees (set with "mtry" argument). The number of trees has linear effect on computation time: reducing it twice will result in double the speed. Reductions in "mtry", on the other hand, give more speed-up when there are more variables in the data.

---

## Speed-accuracy trade-off example

```yaml
type: TwoColumns
key: b8a197e378
code_zoom: 80
```

`@part1`
Default settings:{{1}}
```
> start_time <- Sys.time()
> imp_list <- missForest(iris_miss)
> end_time <- Sys.time()

> print(imp_list$OOBerror)
> print(end_time - start_time)
```
{{1}}
```
     NRMSE        PFC
0.14357394 0.02962963
Time difference of 0.498292 secs
```
{{2}}

`@part2`
Reduced forests:{{3}}
```
> start_time <- Sys.time()
> imp_list <- missForest(iris_miss,
                         ntree = 25,
                         mtry = 2)
> end_time <- Sys.time()

> print(imp_list$OOBerror)
> print(end_time - start_time)
```
{{3}}
```
     NRMSE        PFC
0.15010153 0.05185185
Time difference of 0.193445 secs
```
{{4}}

`@script`
Let's see it in practice. First, let's time the run with default settings, i.e. 100 trees and a square root of the number of variables considered for splits. Now, compare it with the run with only 25 trees and "mtry" equal to 2. Notice how the computation time went down, but the estimated errors increased.

---

## Let's practice!

```yaml
type: FinalSlide
key: dfa7d1547f
```

`@script`
Let's practice what you've learned!
