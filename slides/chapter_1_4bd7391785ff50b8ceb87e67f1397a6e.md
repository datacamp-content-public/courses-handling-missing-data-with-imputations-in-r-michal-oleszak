---
key: 4bd7391785ff50b8ceb87e67f1397a6e
title: 'Insert title here'
---

## Tree-based imputation

```yaml
type: TitleSlide
key: 3970c4b236
```

`@lower_third`
name: MichaÅ‚ Oleszak
title: Machine Learning Engineer

`@script`


---

## Tree-based imputation approach

```yaml
type: FullSlide
key: ee5a81dc7c
disable_transition: false
hide_title: false
```

`@part1`
Idea: use machine learning models based on decision trees to predict missing values
* Non-parametric approach: no assumptions on relationships between variables
* Can pick up complex non-linear patterns
* Often better predictive performance compared to simple statistical models

> 

This course: `missForest` package, based on `randomForest`


`@script`


---

## Recap: random forests

```yaml
type: FullSlide
key: 54bdbd4333
```

`@part1`
* **Decision trees**:{{1}}
	* Choose a variable and split the data into two or more subsets based on its values {{2}}
    * For each subset, choose another variable and again use it for splitting {{3}}
    * Continue splitting, until each subset contains a unique target value {{4}}
    * Splitter variables and thresholds at each step are chosen such that the target variable in each subset is as homogeneous as possible {{5}}

* **Random forests**: {{6}}
	* Take a sample from the data with replacement _(bagging)_ {{7}}
    * Fit a decision tree, considering only a few random variables to split on at each step {{8}}
    * Repeat multiple times and aggregate the results {{9}}

`@script`
trees protect each other from their individual errors 

---

## missForest algorithm

```yaml
type: FullSlide
key: 067795754d
```

`@part1`
1. Make an initial guess for missing values with mean imputation {{1}}
2. Sort variables by the increasing amount of missing values {{2}}
3. While not **STOP**: {{3}}
	* Store previously imputed data matrix {{4}}
	* For each variable **x** (in the sorted order): {{5}}
        * Fit a random forest to predict **x** with other variables, using data where **x** is observed {{6}}
        * Predict missing values in **x** with the trained forest {{7}}
        * Update the imputed data matrix with predictions of **x** {{8}}

**STOP** when the difference between the last two imputed data matrices increases for the first time.{{9}}


`@script`
To begin, make an initial guess for the missing values in X using mean imputation or another imputation method. Then, sort the variables Xs, s = 1, . . . , p according to the amount of missing values
starting with the lowest amount. For each variable Xs the missing values are imputed by first fitting a
random forest with response y
(s)
obs and predictors x
(s)
obs; then, predicting the missing values y
(s)
mis by applying the trained random forest to x
(s)
mis. The imputation procedure is repeated until a stopping criterion
is met.

The stopping criterion is met when the difference between the newly imputed data matrix and the previous one increases for the first time.

---

## missForest example

```yaml
type: TwoColumns
key: 1f0fd38014
```

`@part1`
Introduce missing values to _mtcars_ data:
```
data(mtcars)
mtcars_miss <- prodNA(mtcars, 
                      noNA = 0.1)
sum(is.na(mtcars_miss))
```

```
> [1]
```

`@part2`
Impute with _missForest_:
```
library(missForest)

mtcars_imp <- missForest(mtcars_miss)
sum(is.na(mtcars_imp))
```

```
> [0]
```

`@script`


---

## missForest example

```yaml
type: FullSlide
key: 55c5b41ce0
```

`@part1`
```
data(mtcars)
mtcars_miss <- prodNA(mtcars, noNA = 0.1)
sum(is.na(mtcars_miss))
```

```
> [1]
```

```
library(missForest)
mtcars_imp <- missForest(mtcars_miss)
sum(is.na(mtcars_imp))
```

```
> [0]
```


`@script`


---

## Imputation error

```yaml
type: FullSlide
key: cc9cfe1706
```

`@part1`


`@script`


---

## Speed-accuracy trade-off

```yaml
type: FullSlide
key: 4b8868da1b
```

`@part1`


`@script`


---

## Let's practice!

```yaml
type: FinalSlide
key: dfa7d1547f
```

`@script`
