---
key: 4bd7391785ff50b8ceb87e67f1397a6e
title: 'Insert title here'
---

## Tree-based imputation

```yaml
type: TitleSlide
key: 3970c4b236
```

`@lower_third`
name: Michał Oleszak
title: Machine Learning Engineer

`@script`


---

## Tree-based imputation approach

```yaml
type: FullSlide
key: ee5a81dc7c
disable_transition: false
hide_title: false
```

`@part1`
Idea: use machine learning models based on decision trees to predict missing values
* Non-parametric approach: no assumptions on relationships between variables
* Can pick up complex non-linear patterns
* Often better predictive performance compared to simple statistical models

> 

This course: `missForest` package, based on `randomForest`


`@script`


---

## Recap: random forests

```yaml
type: FullSlide
key: 54bdbd4333
```

`@part1`
* **Decision trees**:{{1}}
	* Choose a variable and split the data into two or more subsets based on its values {{2}}
    * For each subset, choose another variable and again use it for splitting {{3}}
    * Continue splitting, until each subset contains a unique target value {{4}}
    * Splitter variables and thresholds at each step are chosen such that the target variable in each subset is as homogeneous as possible {{5}}

* **Random forests**: {{6}}
	* Take a sample from the data with replacement _(bagging)_ {{7}}
    * Fit a decision tree, considering only a few random variables to split on at each step {{8}}
    * Repeat multiple times and aggregate the results {{9}}

`@script`
trees protect each other from their individual errors 

---

## missForest algorithm

```yaml
type: FullSlide
key: 067795754d
```

`@part1`
1. Make an initial guess for missing values with mean imputation {{1}}
2. Sort variables by the increasing amount of missing values {{2}}
3. While not **STOP**: {{3}}
	* Store previously imputed data matrix {{4}}
	* For each variable **x** (in the sorted order): {{5}}
        * Fit a random forest to predict **x** with other variables, using data where **x** is observed {{6}}
        * Predict missing values in **x** with the trained forest {{7}}
        * Update the imputed data matrix with predictions of **x** {{8}}

**STOP** when the difference between the last two imputed data matrices increases for the first time.{{9}}


`@script`
To begin, make an initial guess for the missing values in X using mean imputation or another imputation method. Then, sort the variables Xs, s = 1, . . . , p according to the amount of missing values
starting with the lowest amount. For each variable Xs the missing values are imputed by first fitting a
random forest with response y
(s)
obs and predictors x
(s)
obs; then, predicting the missing values y
(s)
mis by applying the trained random forest to x
(s)
mis. The imputation procedure is repeated until a stopping criterion
is met.

The stopping criterion is met when the difference between the newly imputed data matrix and the previous one increases for the first time.

---

## missForest example

```yaml
type: TwoColumns
key: 1f0fd38014
```

`@part1`
Introduce missing values to _mtcars_ data: {{1}}
```
data(mtcars)

mtcars_miss <- prodNA(mtcars, 
                      noNA = 0.1)
```
{{1}}
```
sum(is.na(mtcars_miss))
> [1]
```
{{2}}

`@part2`
Impute with _missForest_: {{3}}
```
library(missForest)

imp_list <- missForest(mtcars_miss)
mtcars_imp <- imp_list$ximp
```
{{3}}
```
sum(is.na(mtcars_imp))
> [0]
```
{{4}}

`@script`


---

## Imputation error

```yaml
type: FullSlide
key: cc9cfe1706
```

`@part1`
_missForest_ provides an out-of-bag (OOB) imputation error estimate:
* normalized root mean squared error (NRMSE) for continuous variables
* proportion of falsely classified entries (PFC) for categorical variables

> 

In both cases good performance leads to a value close to 0 and values around 1 indicate poor performance.

```
mtcars_imp$OOBerror
```

```
     NRMSE        PFC
0.15203479 0.04379562
```

`@script`


---

## Speed-accuracy trade-off

```yaml
type: FullSlide
key: 4b8868da1b
```

`@part1`
Growing a random forest for each variable for many iterations might take a lot of time for large datasets. There are two ways to speed up the process:
* Reduce the number of trees grown in each forest (`ntree` argument)
* Reduce the number of variables randomly sampled at each split (`mtry` argument)

> 

Both are likely to decrease imputation accuracy, but differ in their effect on computation time:
* Reducing `ntree` has linear effect on computation time
* Reducing `mtry` gives a larger speed-up when with many variables

`@script`


---

## Speed-accuracy trade-off example

```yaml
type: FullSlide
key: 33c0bbb1f6
```

`@part1`
The default values are `ntree = 100` and `mtry = √p`,
where _p_ is the number of variables

```
imp_list_default <- missForest(mtcars_miss)
imp_list_defalut$OOBerror
imp_list_defalut$time
```

```
imp_list_fast <- missForest(mtcars_miss, ntree = 50, mtry = 3)
imp_list_fast$OOBerror
imp_list_fast$time
```

`@script`


---

## Let's practice!

```yaml
type: FinalSlide
key: dfa7d1547f
```

`@script`
